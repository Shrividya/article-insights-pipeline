## Article Insights Pipeline

> Automated NLP pipeline powered by Apache Airflow 3 â€” extracts keywords, sentiment, named entities, readability scores, and social snippets from Medium articles. Built with NLTK and TextBlob.

---

## Overview

**Article Insights Pipeline** is a local NLP pipeline that takes a Medium article as input and automatically runs a suite of text analysis tasks in parallel using Apache Airflow 3. Results are saved as a structured JSON file for downstream use in content strategy, SEO, and social media.

---

## Features

| Task | Description |
|---|---|
| ðŸ”‘ Keyword Extraction | Top 10 most frequent meaningful words |
| ðŸ“ Summarization | Extractive summary using sentence scoring |
| ðŸ’¬ Sentiment Analysis | Tone, polarity, and subjectivity scores |
| ðŸ·ï¸ Named Entity Recognition | People, organizations, locations |
| ðŸ“– Readability Score | Flesch-Kincaid score and reading level |
| ðŸ“£ Social Snippets | Auto-generated captions for LinkedIn/Twitter |

---

## Project Structure

```
textflow/
â”œâ”€â”€ airflow-env/               # Virtual environment
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ text_preprocessing.py  # Main Airflow DAG
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/
â”‚   â”‚   â””â”€â”€ article.txt        # Place your Medium article here
â”‚   â””â”€â”€ output/
â”‚       â””â”€â”€ nlp_results.json   # Results saved here after DAG runs
â”œâ”€â”€ plugins/                   # Custom Airflow plugins (if any)
â”œâ”€â”€ logs/                      # Auto-generated by Airflow
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## Setup

### 1. Clone the repository

```bash
git clone https://github.com/username/article-insights-pipeline.git
cd article-insights-pipeline
```

### 2. Create and activate a virtual environment

```bash
python -m venv airflow-env
source airflow-env/bin/activate
```

### 3. Set environment variables

```bash
export AIRFLOW_HOME=$(pwd)
export AIRFLOW__CORE__XCOM_BACKEND=airflow.models.xcom.BaseXCom
```

Add these to your `~/.zshrc` or `~/.bashrc` to persist them:

```bash
echo 'export AIRFLOW_HOME=/path/to/project' >> ~/.zshrc
echo 'export AIRFLOW__CORE__XCOM_BACKEND=airflow.models.xcom.BaseXCom' >> ~/.zshrc
source ~/.zshrc
```

### 4. Install dependencies

```bash
pip install "apache-airflow>=3.0.0"
pip install nltk textblob
pip freeze > requirements.txt
```

### 5. Initialize the database

```bash
airflow db migrate
```
### 6. Run Airflow locally
```
airflow standalone
```

---

##  DAG Architecture

The pipeline loads the article first, then runs all NLP tasks in **parallel**, and finally aggregates results:

```
load_article
     â”‚
     â”œâ”€â”€> extract_keywords
     â”œâ”€â”€> summarize_article
     â”œâ”€â”€> analyze_sentiment   â”€â”€> save_results
     â”œâ”€â”€> extract_entities
     â”œâ”€â”€> compute_readability
     â””â”€â”€> generate_snippets
```

## ðŸ“¦ Dependencies

| Package | Purpose |
|---|---|
| `apache-airflow>=3.0.0` | Pipeline orchestration |
| `nltk` | Tokenization, NER, keyword extraction |
| `textblob` | Sentiment analysis |

---
